<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Megan Chow</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-mchow24/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-sp24-mchow24/hw3/index.html</a></h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>
  During this assignment, I learned how to implement a rendering pipeline from beginning (ray generation) to the end (rendering images by calculating illumination). I used a lot of skills learned in class like ray tracing, BVH algorithms, and efficiency optimizations. The assignment consists of 5 parts. During part 1, I learned how to transform rays from an image into camera and world pace. I also learned how to determine ray-triangle and ray-sphere intersection. From part 2, I learned how to optimize intersection testing through implementation of constructing BVH trees. In part 3, I learned how to implement direct illumination or otherwise one bounce illumination and how it can be done using different sampling methods. In this case, I implemented hemisphere and importance sampling. In part 4, I learned how to do indirect illumination through recursion of new intersections/rays created. Finally, in part 5, I learned how to optimize sampling for lighting through using adaptive sampling to reduce sampling in less complex parts of images.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
  <b>Ray generation</b> takes place in the rendering pipeline when <i>raytrace_pixel</i> calls <i>camera->generate_ray</i> with normalized image coordinates to generate a camera ray, which then gets traced through the scene. 
  <br>
  <br>
  In order to implement <i>generate_ray</i>, I first converted the <i>hFov</i> and <i>vFov</i>, the field of view angles along the X and Y axis, to radians from degrees. Then I established the bottom left and top right corner coordinates of the sensor, which was (-(tan(hFov/2), -tan(vFov/2), -1) and (tan(hFov/2), tan(vFov/2), -1) respectively. I was then able to transform the input image coordinates to the sensor coordinates by calculating the relative location of the normalized input coordinates on the sensor with respect to the width and height of the sensor. I then transformed the camera space coordinates to the world space using the c2w transformation matrix. I normalized the coordinates and then created a new ray with the origin as pos, camera position in world space, and the direction as the calculated world coordinates. 
  <br>
  <br>
  Now with generate_ray, I implemented raytrace_pixel to generate ns_aa random samples on a unit square which indicate where to sample within the input pixel. I normalized these coordinates by the image dimensions and then generated the camera ray from the above generate_ray method. For each ray, I calculate the radiance by using est_radiance_global_illumination and sum then average all samples to update the sampleBuffer with the integral of radiance over the pixel. 
  <br>
  <br>
  For primitive intersection, I implemented ray-triangle and ray-sphere intersection. It fits into our rendering pipeline inside est_radiance_global_illumination since we accumulate the radiance if there is a ray intersection with an object within the BVH. Calculating intersections is important for determining whether rays from the camera intersect with objects in the scene, which indicates whether it is visible to the camera or other light sources. 
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  In order to implement triangle intersection, I started with testing if there exists an intersection in has_intersection. I used the Möller Trumbore algorithm as presented in Lecture 9 (shown below). I first calculated all values on the right of the image which represents the edges of the triangle, given the 3 vertices, as well as the 3 S vectors that we use to test the intersection. I then calculated the dot product of S1 and E1 in order to check if the ray is parallel to the triangle. If so, there is no intersection. Following the Möller Trumbore algorithm, I calculated t, the time of intersection, and b1 and b2 which are barycentric coordinates of the intersection point. From there we can get b0 by using 1-b1-b2. Using the barycentric coordinates, I confirm that the intersection lies within the bounds of the triangle. If so, then if the time is within the range of the ray’s min_t and max_t I can confirm that there is an intersection. 
  <br>
  <br>
  In intersect, I used has_intersection to first check the presence of an appropriate intersection. I then re-calculated the vector produced from the Möller Trumbore algorithm and updated the ray’s new max_t with the intersection t and the t,n,primitive, and bsdf fields in Intersection struct. I calculated the normal vector of the intersection by using the barycentric coordinates and applying them to the triangle’s 3 normal vectors.
</p>

<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/moller_trumbore.png" width="480px" />
          <figcaption align="middle">Möller Trumbore Algorithm</figcaption>
      </tr>
  </table>
</div>
<br>
<h3>Sphere-Intersection Implementation:</h3>
<p>
  <br>
  In order to implement sphere intersection, I also followed the implementation introduced in class (shown below). I first implemented test, and solved for a, b, and c as shown below and then used the quadratic equation to solve for both possible values of t. I then checked to see if there were any valid t within the range of the ray’s min_t and max_t and if so, I filled the input pointers t1 and t2 and returned true for test and has_intersection. I then used these helpers in intersect, where I obtained the intersection times t and used it to update the ray’s max_t and also the field of Intersection struct. I calculated the normal vector by using the ray equation to get r(t) = o + td, and obtaining the vector from this point to the origin of the sphere. 
  <br>
</p>
<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/sphere_intersection.png" width="480px" />
          <figcaption align="middle">Ray-Sphere Intersection Algorithm</figcaption>
      </tr>
  </table>
</div>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBempty.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
      <td>
        <img src="images/banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="images/CBdragon.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  In order to implement construct_bvh, I took a recursive approach which continuously calls construct_bvh until a leaf node can be returned. My base case checks to see if the size of the list of primitives is less than or equal to the max_leaf_size. If so, I set the leaf node pointers correctly and returned the leaf node. If the list is still too large, divide the list of primitives into left and right child nodes and call construct_bvh upon those created nodes. The heuristic I chose for picking the splitting point involved using the longest axis of the bounding box. I used this heuristic because it keeps the bounding box more balanced which increases efficiency of intersection testing and traversal of the BVH tree. I then averaged all of the centroid values within the bounding box at the specific longest axis of which I used as my ‘median’ for partitioning the next left and right child node’s primitives. Here is a demonstration of my splitting heuristic using the cow.dae file:
  <br><br>
  I also implemented BBox::intersect() which goes through each axis/slab of the bounding box and determines the t at which the ray intersects, making sure to also account the direction the ray is going. It also returns false if the ray completely misses the box. I then update the min and max times of intersection.  I use this method when I also implemented BVHAccel::has_intersection and BVHAccel::intersection in order to test whether there exists an intersection in the list of primitives of the BVH and to calculate the closest intersection within the BVH respectively.
  
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt2_maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="images/pt2_CBlucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/wall-e.png" align="middle" width="400px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
      <td>
        <img src="images/peter.png" align="middle" width="400px"/>
        <figcaption>peter.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
  I ran the rendering times experiment using the images shown below (cow, CBcoil, and bench). From running my experiment, I recorded that without using BVH recursion, the cow, CBcoil, and bench took 40s, 51s, and 8min 5s to render respectively. On the contrary and in the same order, with BVH, the times to render took 0.6s, 0.75s, and 1.30s. This is exponentially better in comparison to not using BVH acceleration. The more complex the image becomes the exponentially longer it takes for a program not using BVH acceleration to render. This is because with BVH, we have more efficient ways of checking intersections and not having to go through each primitive within an entire scene. 
</p>
<br>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/cow.png" align="middle" width="350px"/>
        <figcaption>cow.dae</figcaption>
      </td>
      <td>
        <img src="images/CBcoil_bvh.png" align="middle" width="350px"/>
        <figcaption>CBcoil.dae</figcaption>
      </td>
      <td>
        <img src="images/bench_bvh.png" align="middle" width="350px"/>
        <figcaption>bench.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  Before elaborating on the implementations of both functions, overall the direct lighting functions both calculate one-bounce radiance. The workflow is the same, where both methods use samples in order to cast incoming rays and trace inverse rays in order to use the reflectance equation and ultimately the Monte Carlo estimator to estimate radiance.
  <br><br>
  Starting with direct lighting with uniform hemisphere sampling, I implemented estimate_direct_lighting_hemisphere. In this method, I am trying to calculate how much light is reflected back towards the camera which I have to do through estimating how much light arrived at the intersection point. I do this by taking num_samples uniform samples of incoming ray directions in the hemisphere and casting a new ray going from the point of interest, hit_p, in the direction of the sampled direction. I then check to see if this ray intersects with scene primitives by using bvh->intersect(). If not, then I ignore the sample since it indicates that the ray does not add any direct light to the point of interest. If there does exist an intersection, I evaluate the variables used in the reflection equation: the emission from the bvh intersection bsdf, the reflection from the current surface, and cos theta which is the angle between the incoming ray and the surface normal. I then plug these values into the Monte Carlo estimator, with the pdf of 1/(2*PI) for hemisphere sampling, and normalize by the num_samples in order to get the estimated radiance. 
  <br><br>
  In order to implement direct lighting with importance sampling, I implemented estimate_direct_lighting_importance. Unlike hemisphere sampling, I sample the lights directly. I iterate through scene->lights and test to see if the light is a light source. If so, then I change the number of samples to be 1 since the samples from a light source will have the same contribution, otherwise it is defaulted to ns_area_light, number of samples per area light source. For each sample, I call sample_L on the light in order to retrieve the vector giving the sampled direction between p and the light source, the distance between p and the light source, and a pdf. First using the direction produced, I check to ensure that the z-value is non-negative, since we do not want to cast a ray for a direction that points away from the surface and ultimately is behind the surface at the hit point. I then cast a new ray with origin at hit point, the direction of that produced from sample_L, max_t of distance to light, and min_t of an epsilon value to account for precision errors. I then check to ensure that the ray does not intersect with any scene objects to ensure that the light source does cast light onto the hit point. After passing this check, I once again use the reflection equation and the Monte Carlo estimator in order to calculate the radiance. 
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/pt3_CBbunny_H_16_8.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/pt3_CBbunny_16_8.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/pt3_CBspheres_H_16_8.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/pt3_CBspheres_16_8.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt3_soft_bunny_1_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt3_soft_bunny_1_4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt3_soft_bunny_1_16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt3_soft_bunny_1_64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  The noise significantly decreases as we increase the number of light rays. When using 1 light ray, you can observe that the shadows are very rough and the same color, whereas by using 64 light rays, the shadow becomes much more blended and it is clear to differentiate the softer shadows and the shadow becomes lighter. 
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  From a visual standpoint, it is clear that hemisphere sampling results in a lot more noise/graininess while lighting sampling results in a more accurate image. The cause of this is because in hemisphere sampling, we uniformly sample across the hemisphere regardless of where the light sources are and we weight all of the samples equally. However, in importance sampling, we take into account the light in the scene and allocate more samples towards those in order to get a more accurate representation of the lighting. 
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  In order to implement the indirect lighting function, I recursively called at_least_one_bounce_radiance while decrementing the ray depth each time until the depth is 0. I start with using one_bounce_radiance on the current ray and then I use sample_f in order to retrieve the new sample incoming ray direction, wi, the pdf, and the proportion of incoming light scattered from wi to wo, the outgoing solid angle. I convert wi to the world space and cast a new ray from the hit point. If there is an intersection from the bvh, then I add at_least_one_bounce_radiance() result from the generated ray to the output. 

  In order to add the implementation of Russian Roulette, I used the coin_flip method in order to generate true with a probability of 0.35. If it is true, I do early termination, which allows for a more efficient rendering time because it is computationally exhaustive to integrate over all paths of all lengths. 
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt4_bunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/pt4_spheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt4_CBspheres_direct.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt4_spheres.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    When we incorporate indirect illumination, we can see that there is blending of color into the shadow of the image. It takes into account light that comes in from other sources in the scene like the walls (which is where the colors are coming from). On the contrary, direct lighting only takes into account light from a direct light source without any intermediate bounces.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt4_russian_m0_bunny.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt4_russian_m1_bunny.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt4_russian_m2_bunny.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt4_russian_m3_bunny.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt4_russian_m100_bunny.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As we increase the maximum ray depth, we go through a higher depth of recursion which allows us to calculate more indirect illumination. As a resut, we get a more defined and accurate image as we increase the max ray depth.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt4_spheres_s_1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt4_spheres_s_2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt4_spheres_s_4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt4_spheres_s_8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt4_spheres_s_16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt4_spheres_s_64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt4_spheres_s_1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As we increase the samples per pixel, we get a clearer and smoother image since we are able to get a more accurate representation of the lighting with more samples.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  Adaptive sampling allows us to speed up rendering times by not using an equal number of samples per pixel, but rather concentrate sampling more pixels in more complex parts of images. In order to implement this we keep track of two values during each sample in raytrace_pixel which is the illuminance and illuminance^2, which we can get from calling .illum() on our result from est_radiance_global_illumination. WIth these values, for every batch of samples (which determines how often we calculate whether to stop sampling), we calculate the mean and the variance of the illumination. From these values we can calculate an I value = 1.96 * (sqrt(variance/num_samples_so_far). If this value is smaller than the maxTolerance*mean, we can conclude that the pixel has converged. Therefore, we stop sampling and update the pixel with the current summed radiance divided by this new number of samples and also update the sampleCountBuffer to produce our rate images below. 
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/pt5_bunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt5_bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/pt5_spheres.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/pt5_spheres_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
